## Summary of Temporal Graph Learning and Continuous Recommendation System

### Graph Learning Methods (Network-based)

Graph Attention:

| Query       | Center-Node                            |      |
| ----------- | -------------------------------------- | ---- |
| Key & Value | Neighborhood Nodes based on connection |      |

Temporal:

Discrete: Aggregating a series of embeddings with concatenation, MLP, RNN or attention.

Continuous: Encode temporal difference with parameterized cosine embedding. Aggregate by attention.

Usually, use 2 separate model with identical framework to embed items and users.

### Paper Overview

#### Temporally evolving graph neural network for fake news detection

Problem：Binary classification

Dataset：Continuous Graph; *Weibo & Twitter*

Experiment Metric: Accuracy, Precision, Recall, F1-score

Ideas：

1. Temporal Self-Attention in Graph Neighborhood

Data Input: Content semantics (Word2Vec), graph structure(Attention in neighborhood), temporal information(cosine embedding)

##### Proposed Model

For **each timestamp t**, 

​		*Update embedding*

1. Get raw node embedding by Word2Vec(BERT; dim=768) : H(t)

2. Update node embedding by **temporal graph attention**, take H(t) add memory embedding S(t) as input, get $\hat{H}(t)$

3. Update S(t) by GRU (one kind of RNN)

   *Aggregate information for prediction*

4. Update node embedding by **graph convolution** layer

5. Aggregate node embedding by **average pooling**, pipe into a FFN and softmax for prediction. Use cross-entropy loss.

Specifically, use TDN to make **temporal graph attention** focus on variational information.

| Temporal graph attention         | $Neigh(v_i^{t_i},t)$= All nodes that have interaction with $v_i$ before t |
| -------------------------------- | ------------------------------------------------------------ |
| TDN: Temporal difference network | $L_d=\frac{1}{n-1}\sum\limits_{1\leq i\leq n-1} \Phi(meanPooling(\hat{H}_i(t),\hat H_{i+1}(t)))$, $\Phi$ is cosine similarity (inner product) |

Training Strategy

1. $\min L(\Theta) = L_C(\Theta)+\lambda L_d$
2. When converged, freeze embedding layer and remove TDN
3. Use Adam.



#### Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting

##### Model

Three GCN based on different time interval.



#### Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer

##### Idea: Unify sequential patterns and temporal collaborative signals

##### Dataset: ML-100k & Amazon

##### Model:

1. A transformer-based model

2. Two attention module (parallel-design):

   1. Initially, we get embedding of users and items

   2. Then, from users' perspective, we calculate the new embedding of user by using itself to quert its item history, i.e. new embedding of a user is the weighted average of it's historical shopped items' embedding;

   3. Similarly, we get the new embedding of items.

   4. The attention includes time attention;

   5. After several layers of this attention block, we get the embedding of users and items at time T; then use dot product to get the rank list for every user.

   6. Loss function

      1. pairwise BPR loss or Binary Cross Entropy loss

         The motivation is that observed data should rank higher than the unobserved data
         
##### Code
1. Data Preprocessing
	1. Preprocess of ml-100k : 
	2. Reindx all users and items;
	3. Items' indices are following users' indices;
	4. 3 Diffierent Data Representation
		1. table data
		2. adjancy "matrix" (list of list, different nodes has different amounts of neighbors)
		3. Sparse Matrix
			1. node list
			2. offset list
			3. neighbor list
			4. time stamp list
			5. edge index list
2. Data Loader
	1. NeighborFinder
		1. Use all sparse matrix data during initialization
		2. Take a batch of data (part of the adjancy matrix)
		3. return the neigbors before the transaction date \[batch_size, neighbor_num]
		4. NeighborFinder is included in the model for conviniently searching for neighbors; It has same input as the model (part of adjancy matrix)
3. Model
	1. Each epoch iterates the whole adjancy matrix; Each batch iterates part of it.
	2. In first layer, use simple embedding of users and items.
	3. In following layers, use (current_layer - 1)'s embedding of users and items as input for attention module, then get the new embedding as output of current layer. (**Note: Each layer's output has shape (batch_size, node_dim), however it already has time embedded in it, except for the first layer; However, in each layer, the model explictly takes time embedding as input in the attention module**)

#### Predicting Dynamic Embedding Trajectory in Temporal Interaction

##### Model

 2 separate RNN：update user's embedding by its last embedding, its last-interacted item's embedding

projection operator: estimate future embedding

T-batch: Update uncorrelated nodes at the same time



#### ROLAND Graph Learning Framework for Dynamic Graphs

<u>**Unsolved Problem**：coarse grained temporal information, discrete model (using snapshots instead of data stream)</u>

This paper proposed a framework for **predicting edge labels(0 or 1) of dynamic graphs.**

Dataset: Financial Transaction & Social Network & Traffic 

Metric: Mean Reciprocal Rank

Details

- Use hierarachical historical embeddings**, instead of just updating the top layer's embeddings.

- By this means, it's incremental learning, inrelevant to the total length of time period, **saving GPU memory**.

- It employs **meta-learning**, treat each period's task as different task;
	- The example in the paper is about traffic predicting: traffic in Fri and Sat are different;
	- This paper solves this problem by meta-learning, in ASTGCN, they solve it by fusion of daily, weekly and monthly parts; The former one may be better when you do NOT know the implict pattern of the data.
	- Meta-Learning: Instead of learning a model for all tasks (e.g., predict everyday traffic), learn a model for initialization. For each new task, compute the moving average of newly trained model and the meta-model.
	
- Training Skill

  **Live-update**: In **each snapshot**, we train on 90% edges, and use the remain 10% as evaluating (can serve as criterion for early-stop); Instead of the traditional way, that use the last 10% snapshot for evaluating. (Notes: This is like nlp, use the ground-truth next token and predicted next token to calculate loss in each predicting step)
